{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v4ouCqnzJDE",
        "outputId": "7e0f4caa-889c-41e3-8922-8796adeadf07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ],
      "metadata": {
        "id": "JCY1KWymzUjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList, BatchNorm1d\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "metadata": {
        "id": "Kd6VkbsR0UVk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess dataset\n"
      ],
      "metadata": {
        "id": "_P2Nn45r0bd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"ogbn-arxiv\"\n",
        "dataset = PygNodePropPredDataset(name = dataset_name, transform=T.ToSparseTensor())"
      ],
      "metadata": {
        "id": "OZu_q7qj0a1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d2fddc-7c95-4d62-af32-60313358dd19"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:10<00:00,  7.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 8112.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1697.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])\n",
        "data = dataset[0]\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = data.to(device)\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eZDsHON07-b",
        "outputId": "ec5e459c-cca3-4b8e-fd3b-a46ae5c54df9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "        for i in range(num_layers):\n",
        "          if i == 0:\n",
        "            self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "          elif i == num_layers :\n",
        "            self.convs.append(GCNConv(hidden_dim, output_dim))\n",
        "          else:\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.bns = ModuleList()\n",
        "        for i in range(num_layers -1):\n",
        "         self.bns.append(BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.softmax = torch.nn.LogSoftmax()\n",
        "          \n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs):\n",
        "          if i < len(self.convs) -1:\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout)\n",
        "          else:\n",
        "            if self.return_embeds:\n",
        "              x = conv(x, adj_t)\n",
        "            else:\n",
        "              x = conv(x, adj_t)\n",
        "              x = self.softmax(x)\n",
        "        out = x\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "HtyMrl2b1lXL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "  model.train()\n",
        "  loss = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  output = model(data.x, data.adj_t)\n",
        "  output_train = output[train_idx]\n",
        "  output_train_label = data.y[train_idx]\n",
        "  loss = loss_fn(output_train, output_train_label.reshape(-1))\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.item()\n"
      ],
      "metadata": {
        "id": "4IV5_m2nT_MN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions\")\n",
        "\n",
        "      data = {}\n",
        "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
        "\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "metadata": {
        "id": "hX1oGPQ9WrTc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "      'device': device,\n",
        "      'num_layers': 3,\n",
        "      'hidden_dim': 256,\n",
        "      'dropout': 0.5,\n",
        "      'lr': 0.01,\n",
        "      'epochs': 100,\n",
        "}"
      ],
      "metadata": {
        "id": "xFDTNUHyT_Hj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(data.num_features, args['hidden_dim'],\n",
        "              dataset.num_classes, args['num_layers'],\n",
        "              args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ],
      "metadata": {
        "id": "Vbcw5mkfXQvk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_parameters()\n",
        "import copy\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7lU9UyAXZhN",
        "outputId": "ddf6696e-7fd7-42f2-87bf-5c0e0fab9254"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 5.9345, Train: 24.34%, Valid: 28.74% Test: 25.88%\n",
            "Epoch: 02, Loss: 3.1830, Train: 36.28%, Valid: 45.48% Test: 48.39%\n",
            "Epoch: 03, Loss: 2.2471, Train: 36.88%, Valid: 36.69% Test: 40.46%\n",
            "Epoch: 04, Loss: 1.8740, Train: 37.35%, Valid: 34.18% Test: 35.12%\n",
            "Epoch: 05, Loss: 1.6785, Train: 34.59%, Valid: 28.41% Test: 27.04%\n",
            "Epoch: 06, Loss: 1.5625, Train: 33.78%, Valid: 25.35% Test: 23.04%\n",
            "Epoch: 07, Loss: 1.4677, Train: 34.15%, Valid: 26.19% Test: 23.55%\n",
            "Epoch: 08, Loss: 1.3995, Train: 35.39%, Valid: 29.31% Test: 27.26%\n",
            "Epoch: 09, Loss: 1.3547, Train: 37.86%, Valid: 33.35% Test: 31.50%\n",
            "Epoch: 10, Loss: 1.3107, Train: 40.49%, Valid: 38.07% Test: 38.20%\n",
            "Epoch: 11, Loss: 1.2791, Train: 43.48%, Valid: 42.82% Test: 44.99%\n",
            "Epoch: 12, Loss: 1.2534, Train: 45.78%, Valid: 45.75% Test: 48.72%\n",
            "Epoch: 13, Loss: 1.2324, Train: 47.81%, Valid: 47.25% Test: 50.41%\n",
            "Epoch: 14, Loss: 1.2149, Train: 50.95%, Valid: 51.08% Test: 53.37%\n",
            "Epoch: 15, Loss: 1.1970, Train: 53.73%, Valid: 55.07% Test: 56.55%\n",
            "Epoch: 16, Loss: 1.1860, Train: 55.59%, Valid: 56.57% Test: 58.22%\n",
            "Epoch: 17, Loss: 1.1733, Train: 56.29%, Valid: 56.79% Test: 58.95%\n",
            "Epoch: 18, Loss: 1.1650, Train: 56.34%, Valid: 57.07% Test: 59.00%\n",
            "Epoch: 19, Loss: 1.1517, Train: 55.80%, Valid: 56.23% Test: 58.59%\n",
            "Epoch: 20, Loss: 1.1408, Train: 55.71%, Valid: 55.80% Test: 57.91%\n",
            "Epoch: 21, Loss: 1.1416, Train: 56.09%, Valid: 55.92% Test: 57.79%\n",
            "Epoch: 22, Loss: 1.1291, Train: 57.17%, Valid: 56.80% Test: 58.88%\n",
            "Epoch: 23, Loss: 1.1173, Train: 58.53%, Valid: 58.29% Test: 60.18%\n",
            "Epoch: 24, Loss: 1.1132, Train: 59.50%, Valid: 58.77% Test: 61.08%\n",
            "Epoch: 25, Loss: 1.1058, Train: 60.51%, Valid: 60.35% Test: 62.33%\n",
            "Epoch: 26, Loss: 1.0979, Train: 61.20%, Valid: 60.75% Test: 62.72%\n",
            "Epoch: 27, Loss: 1.0914, Train: 62.32%, Valid: 61.77% Test: 63.43%\n",
            "Epoch: 28, Loss: 1.0827, Train: 63.24%, Valid: 62.62% Test: 64.25%\n",
            "Epoch: 29, Loss: 1.0785, Train: 64.06%, Valid: 63.86% Test: 64.87%\n",
            "Epoch: 30, Loss: 1.0742, Train: 65.03%, Valid: 65.06% Test: 65.54%\n",
            "Epoch: 31, Loss: 1.0696, Train: 65.43%, Valid: 65.55% Test: 66.25%\n",
            "Epoch: 32, Loss: 1.0628, Train: 66.07%, Valid: 66.36% Test: 66.99%\n",
            "Epoch: 33, Loss: 1.0576, Train: 66.20%, Valid: 66.50% Test: 67.28%\n",
            "Epoch: 34, Loss: 1.0533, Train: 66.55%, Valid: 66.57% Test: 67.38%\n",
            "Epoch: 35, Loss: 1.0461, Train: 66.81%, Valid: 66.79% Test: 67.28%\n",
            "Epoch: 36, Loss: 1.0443, Train: 67.04%, Valid: 67.11% Test: 67.63%\n",
            "Epoch: 37, Loss: 1.0396, Train: 67.28%, Valid: 67.73% Test: 67.91%\n",
            "Epoch: 38, Loss: 1.0344, Train: 67.97%, Valid: 68.27% Test: 68.36%\n",
            "Epoch: 39, Loss: 1.0334, Train: 68.40%, Valid: 68.26% Test: 68.64%\n",
            "Epoch: 40, Loss: 1.0273, Train: 68.68%, Valid: 68.92% Test: 68.72%\n",
            "Epoch: 41, Loss: 1.0205, Train: 68.92%, Valid: 68.68% Test: 68.03%\n",
            "Epoch: 42, Loss: 1.0213, Train: 69.00%, Valid: 68.58% Test: 67.90%\n",
            "Epoch: 43, Loss: 1.0185, Train: 68.91%, Valid: 68.44% Test: 67.71%\n",
            "Epoch: 44, Loss: 1.0142, Train: 68.76%, Valid: 68.19% Test: 68.03%\n",
            "Epoch: 45, Loss: 1.0109, Train: 68.90%, Valid: 68.24% Test: 68.45%\n",
            "Epoch: 46, Loss: 1.0077, Train: 69.12%, Valid: 68.68% Test: 68.58%\n",
            "Epoch: 47, Loss: 1.0056, Train: 69.33%, Valid: 69.05% Test: 68.77%\n",
            "Epoch: 48, Loss: 1.0039, Train: 69.42%, Valid: 69.08% Test: 68.57%\n",
            "Epoch: 49, Loss: 0.9962, Train: 69.06%, Valid: 68.86% Test: 68.74%\n",
            "Epoch: 50, Loss: 0.9942, Train: 68.87%, Valid: 68.72% Test: 68.33%\n",
            "Epoch: 51, Loss: 0.9923, Train: 69.59%, Valid: 68.88% Test: 68.42%\n",
            "Epoch: 52, Loss: 0.9913, Train: 69.91%, Valid: 68.96% Test: 68.25%\n",
            "Epoch: 53, Loss: 0.9877, Train: 70.14%, Valid: 69.28% Test: 68.50%\n",
            "Epoch: 54, Loss: 0.9884, Train: 70.06%, Valid: 69.29% Test: 68.42%\n",
            "Epoch: 55, Loss: 0.9812, Train: 70.24%, Valid: 69.42% Test: 68.74%\n",
            "Epoch: 56, Loss: 0.9784, Train: 70.11%, Valid: 69.77% Test: 69.19%\n",
            "Epoch: 57, Loss: 0.9792, Train: 70.16%, Valid: 69.37% Test: 69.31%\n",
            "Epoch: 58, Loss: 0.9757, Train: 70.13%, Valid: 69.77% Test: 69.33%\n",
            "Epoch: 59, Loss: 0.9711, Train: 70.27%, Valid: 69.74% Test: 69.55%\n",
            "Epoch: 60, Loss: 0.9703, Train: 70.50%, Valid: 69.67% Test: 68.77%\n",
            "Epoch: 61, Loss: 0.9706, Train: 70.68%, Valid: 69.73% Test: 68.53%\n",
            "Epoch: 62, Loss: 0.9676, Train: 70.69%, Valid: 69.68% Test: 68.05%\n",
            "Epoch: 63, Loss: 0.9649, Train: 70.63%, Valid: 69.85% Test: 68.84%\n",
            "Epoch: 64, Loss: 0.9628, Train: 70.58%, Valid: 69.97% Test: 69.29%\n",
            "Epoch: 65, Loss: 0.9598, Train: 70.45%, Valid: 69.85% Test: 69.53%\n",
            "Epoch: 66, Loss: 0.9598, Train: 70.64%, Valid: 69.94% Test: 69.29%\n",
            "Epoch: 67, Loss: 0.9581, Train: 70.78%, Valid: 69.78% Test: 68.87%\n",
            "Epoch: 68, Loss: 0.9553, Train: 70.75%, Valid: 70.22% Test: 69.04%\n",
            "Epoch: 69, Loss: 0.9535, Train: 70.87%, Valid: 69.91% Test: 69.36%\n",
            "Epoch: 70, Loss: 0.9514, Train: 70.92%, Valid: 70.08% Test: 69.42%\n",
            "Epoch: 71, Loss: 0.9485, Train: 70.82%, Valid: 70.35% Test: 69.39%\n",
            "Epoch: 72, Loss: 0.9467, Train: 70.99%, Valid: 70.30% Test: 69.51%\n",
            "Epoch: 73, Loss: 0.9445, Train: 71.12%, Valid: 70.11% Test: 70.07%\n",
            "Epoch: 74, Loss: 0.9428, Train: 71.07%, Valid: 70.26% Test: 69.67%\n",
            "Epoch: 75, Loss: 0.9408, Train: 71.33%, Valid: 70.32% Test: 69.46%\n",
            "Epoch: 76, Loss: 0.9423, Train: 71.35%, Valid: 69.78% Test: 68.67%\n",
            "Epoch: 77, Loss: 0.9374, Train: 71.40%, Valid: 70.00% Test: 69.09%\n",
            "Epoch: 78, Loss: 0.9380, Train: 71.21%, Valid: 69.99% Test: 69.59%\n",
            "Epoch: 79, Loss: 0.9323, Train: 71.35%, Valid: 70.51% Test: 69.71%\n",
            "Epoch: 80, Loss: 0.9306, Train: 71.49%, Valid: 70.21% Test: 68.95%\n",
            "Epoch: 81, Loss: 0.9311, Train: 71.40%, Valid: 69.88% Test: 68.56%\n",
            "Epoch: 82, Loss: 0.9287, Train: 71.51%, Valid: 70.17% Test: 69.07%\n",
            "Epoch: 83, Loss: 0.9273, Train: 71.57%, Valid: 70.11% Test: 69.34%\n",
            "Epoch: 84, Loss: 0.9254, Train: 71.37%, Valid: 70.26% Test: 69.67%\n",
            "Epoch: 85, Loss: 0.9230, Train: 71.47%, Valid: 70.43% Test: 69.47%\n",
            "Epoch: 86, Loss: 0.9214, Train: 71.45%, Valid: 70.03% Test: 68.92%\n",
            "Epoch: 87, Loss: 0.9190, Train: 71.66%, Valid: 70.26% Test: 68.92%\n",
            "Epoch: 88, Loss: 0.9172, Train: 71.61%, Valid: 70.45% Test: 69.68%\n",
            "Epoch: 89, Loss: 0.9176, Train: 71.51%, Valid: 70.46% Test: 70.04%\n",
            "Epoch: 90, Loss: 0.9169, Train: 71.49%, Valid: 70.43% Test: 70.17%\n",
            "Epoch: 91, Loss: 0.9150, Train: 71.79%, Valid: 70.51% Test: 70.07%\n",
            "Epoch: 92, Loss: 0.9132, Train: 71.75%, Valid: 70.36% Test: 69.53%\n",
            "Epoch: 93, Loss: 0.9118, Train: 71.71%, Valid: 70.30% Test: 69.38%\n",
            "Epoch: 94, Loss: 0.9090, Train: 71.75%, Valid: 70.33% Test: 69.71%\n",
            "Epoch: 95, Loss: 0.9090, Train: 71.77%, Valid: 70.17% Test: 69.58%\n",
            "Epoch: 96, Loss: 0.9088, Train: 72.04%, Valid: 70.42% Test: 69.79%\n",
            "Epoch: 97, Loss: 0.9072, Train: 71.96%, Valid: 70.65% Test: 69.18%\n",
            "Epoch: 98, Loss: 0.9055, Train: 71.70%, Valid: 69.74% Test: 68.28%\n",
            "Epoch: 99, Loss: 0.9023, Train: 71.80%, Valid: 70.06% Test: 69.56%\n",
            "Epoch: 100, Loss: 0.9021, Train: 71.75%, Valid: 70.44% Test: 70.58%\n"
          ]
        }
      ]
    }
  ]
}